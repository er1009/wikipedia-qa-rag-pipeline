{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Stage RAG Pipeline for Wikipedia QA\n",
        "\n",
        "```\n",
        "Stage 0: Query2Doc Expansion   → LLM generates contextual keywords\n",
        "Stage 1: BM25 Retrieval        → Document retrieval + contextual chunking + passage filtering\n",
        "Stage 2: Bi-Encoder Reranking  → Semantic filtering (200 → 25 passages)\n",
        "Stage 3: Cross-Encoder Rerank  → Precision reranking (25 → 5 passages)\n",
        "Stage 4: LLM Generation        → Answer extraction with Llama-3.2-1B\n",
        "Stage 5: Post-Processing       → Self-consistency voting + F1 optimization\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java (required for Pyserini)\n",
        "!apt-get update -qq && apt-get install -y -qq openjdk-21-jdk-headless > /dev/null 2>&1\n",
        "\n",
        "# Install Python dependencies\n",
        "!pip install -q pyserini==0.36.0 torch transformers accelerate sentence-transformers \\\n",
        "    rank-bm25 langchain-text-splitters pandas numpy tqdm huggingface_hub datasets gdown\n",
        "\n",
        "# Optional: Flash Attention for faster LLM inference on A100/H100\n",
        "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo 'Flash Attention not available (CPU/T4)'\n",
        "\n",
        "print('\\n✓ All dependencies installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repo (or mount Google Drive with the repo)\n",
        "!git clone https://github.com/er1009/wikipedia-qa-rag-pipeline.git 2>/dev/null || echo 'Repo already cloned'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'wikipedia-qa-rag-pipeline')\n",
        "\n",
        "# Configure logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s | %(name)s | %(message)s')\n",
        "\n",
        "# Verify imports\n",
        "from src.pipeline import RAGPipeline\n",
        "from src.config import FAST_CONFIG, BALANCED_CONFIG, COMPETITION_CONFIG\n",
        "from src.metrics import f1_score, max_over_ground_truths\n",
        "print('✓ Modules imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option 1: Set your token here\n",
        "# HF_TOKEN = 'hf_your_token_here'\n",
        "\n",
        "# Option 2: Use environment variable (recommended)\n",
        "HF_TOKEN = os.environ.get('HF_TOKEN', None)\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print('✓ Logged in to Hugging Face')\n",
        "else:\n",
        "    print('⚠ Set HF_TOKEN or uncomment Option 1 above')\n",
        "    print('  Get a token at: https://huggingface.co/settings/tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Download data (update this link to your data source)\n",
        "DATA_URL = 'https://drive.google.com/drive/folders/1N2r7REIvn6pmnHyx_PSrERJhqMIF736S?usp=drive_link'\n",
        "!gdown --folder $DATA_URL -O ./data/ -q 2>/dev/null || echo 'Download data manually into ./data/'\n",
        "\n",
        "DATA_PATH = './data/rag_course'\n",
        "df_train = pd.read_csv(f'{DATA_PATH}/train.csv', converters={'answers': json.loads})\n",
        "df_test = pd.read_csv(f'{DATA_PATH}/test.csv', converters={'answers': json.loads})\n",
        "\n",
        "print(f'✓ Train: {len(df_train)} samples')\n",
        "print(f'✓ Test:  {len(df_test)} samples')\n",
        "print(f'\\nExample: {df_train[\"question\"].iloc[0]}')\n",
        "print(f'Answers: {df_train[\"answers\"].iloc[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Download BM25 Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "# Mount Google Drive for persistent index storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "INDEX_NAME = 'wikipedia-kilt-doc'\n",
        "DRIVE_DIR = '/content/drive/MyDrive/pyserini'\n",
        "LOCAL_INDEX = f'/content/{INDEX_NAME}'\n",
        "\n",
        "# Download index via Pyserini (cached on Drive after first run)\n",
        "if not os.path.isdir(LOCAL_INDEX):\n",
        "    print(f'Downloading {INDEX_NAME} index (first time only, ~30GB)...')\n",
        "    searcher = LuceneSearcher.from_prebuilt_index(INDEX_NAME)\n",
        "    LOCAL_INDEX = searcher.index_dir\n",
        "    print(f'✓ Index downloaded to: {LOCAL_INDEX}')\n",
        "else:\n",
        "    print(f'✓ Index found at: {LOCAL_INDEX}')\n",
        "\n",
        "BM25_INDEX_PATH = LOCAL_INDEX\n",
        "print(f'\\nBM25 index path: {BM25_INDEX_PATH}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = RAGPipeline(\n",
        "    bm25_index_path=BM25_INDEX_PATH,\n",
        "    bi_encoder_model='sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
        "    cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
        "    llm_model='meta-llama/Llama-3.2-1B-Instruct',\n",
        "    device='cuda',\n",
        "    use_flash_attn=True,\n",
        ")\n",
        "\n",
        "print('\\n✓ Pipeline ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quick Test (5 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity check with FAST_CONFIG\n",
        "sample = df_train.head(5)\n",
        "\n",
        "results = pipeline.evaluate(\n",
        "    questions=sample['question'].tolist(),\n",
        "    ground_truths=sample['answers'].tolist(),\n",
        "    config=FAST_CONFIG,\n",
        ")\n",
        "\n",
        "# Show predictions\n",
        "for i in range(len(sample)):\n",
        "    q = sample['question'].iloc[i]\n",
        "    gt = sample['answers'].iloc[i]\n",
        "    pred = results['predictions'][i]\n",
        "    f1 = results['f1_scores'][i]\n",
        "    print(f'\\nQ: {q[:80]}...')\n",
        "    print(f'  GT:   {gt}')\n",
        "    print(f'  Pred: {pred}')\n",
        "    print(f'  F1:   {f1:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Full Evaluation (Competition Config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on training set with full pipeline\n",
        "EVAL_SAMPLES = 100  # Set to None for full training set\n",
        "\n",
        "sample = df_train.head(EVAL_SAMPLES) if EVAL_SAMPLES else df_train\n",
        "\n",
        "results = pipeline.evaluate(\n",
        "    questions=sample['question'].tolist(),\n",
        "    ground_truths=sample['answers'].tolist(),\n",
        "    config=COMPETITION_CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Generate Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for test set\n",
        "test_queries = df_test['question'].tolist()\n",
        "test_ids = df_test['id'].tolist()\n",
        "\n",
        "answers = pipeline.answer(test_queries, config=COMPETITION_CONFIG)\n",
        "\n",
        "# Save submission CSV\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'prediction': [json.dumps([a], ensure_ascii=False) for a in answers],\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(f'\\n✓ Saved {len(submission)} predictions to submission.csv')\n",
        "submission.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
